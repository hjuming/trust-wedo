from typing import List, Dict, Any, Tuple
from app.models.signals import SiteSignals
from app.services.scoring import calculate_weighted_score, score_to_grade, calculate_dimension_scores
from app.services.site_classifier import classify_site_type, generate_custom_suggestions
from app.config.difficult_sites import check_difficult_site, get_estimated_dimensions

class Rule:
    """å–®ä¸€è¦å‰‡"""
    def __init__(self, rule_id: str, condition, issue: Dict[str, Any], suggestion: Dict[str, Any]):
        self.rule_id = rule_id
        self.condition = condition
        self.issue = issue
        self.suggestion = suggestion
    
    def evaluate(self, signals: SiteSignals) -> bool:
        """è©•ä¼°è¦å‰‡æ˜¯å¦å‘½ä¸­"""
        return self.condition(signals)

class ReportEngine:
    """å ±å‘Šå¼•æ“ v2.0 - å¼·åŒ–ç‰ˆ"""
    
    VERSION = "r2.0"
    
    def __init__(self):
        self.rules = self._init_rules()
    
    def _init_rules(self) -> List[Rule]:
        """åˆå§‹åŒ–è¦å‰‡é›†"""
        return [
            Rule(
                rule_id="R001",
                condition=lambda s: not s.has_title,
                issue={
                    "severity": "high",
                    "title": "ç¶²ç«™æ²’æœ‰æ¸…æ¥šçš„æ¨™é¡Œ",
                    "description": "AI èˆ‡æœå°‹å¼•æ“ç„¡æ³•å¿«é€Ÿè­˜åˆ¥ä½ çš„ç¶²ç«™ä¸»é¡Œï¼Œé€™æœƒé™ä½æ¬Šå¨æ„Ÿã€‚",
                    "why": "æ¨™é¡Œæ˜¯ AI ç†è§£ç¶²ç«™çš„ç¬¬ä¸€æ­¥"
                },
                suggestion={
                    "priority": "high",
                    "effort": "easy",
                    "impact": "high",
                    "action": "åœ¨ç¶²é åŸå§‹ç¢¼ä¸­æ–°å¢ <title> æ¨™ç±¤",
                    "impact_desc": "ç«‹å³æå‡æœå°‹å¼•æ“æ’åèˆ‡ AI å¼•ç”¨ç‡",
                    "how_to": [
                        "1. æ‰“é–‹ç¶²ç«™çš„ HTML æª”æ¡ˆ",
                        "2. åœ¨ <head> å€å¡Šä¸­åŠ å…¥ <title>ä½ çš„ç¶²ç«™åç¨±</title>",
                        "3. ç¢ºä¿æ¨™é¡Œç°¡æ½”ä¸”æè¿°æ€§å¼·ï¼ˆå»ºè­° 50-60 å­—å…ƒï¼‰",
                        "4. å„²å­˜ä¸¦é‡æ–°éƒ¨ç½²ç¶²ç«™"
                    ]
                }
            ),
            Rule(
                rule_id="R002",
                condition=lambda s: not s.has_description,
                issue={
                    "severity": "medium",
                    "title": "ç¼ºå°‘ç¶²ç«™æè¿°",
                    "description": "ç¼ºå°‘ Meta Description è®“ AI é›£ä»¥åœ¨ä¸é–±è®€å…¨ç«™å…§å®¹çš„æƒ…æ³ä¸‹ç¸½çµä½ çš„åƒ¹å€¼ã€‚",
                    "why": "æè¿°å¹«åŠ© AI å¿«é€Ÿæ‘˜è¦ç¶²ç«™å…§å®¹"
                },
                suggestion={
                    "priority": "medium",
                    "effort": "easy",
                    "impact": "medium",
                    "action": "æ’°å¯«ä¸¦åŠ å…¥ <meta name='description'> æ¨™ç±¤",
                    "impact_desc": "æé«˜æœå°‹çµæœçš„é»æ“Šç‡ä¸¦å¹«åŠ© AI æ‘˜è¦",
                    "how_to": [
                        "1. æ’°å¯« 150-160 å­—å…ƒçš„ç¶²ç«™æè¿°",
                        "2. åœ¨ <head> å€å¡ŠåŠ å…¥ <meta name='description' content='ä½ çš„æè¿°'>",
                        "3. ç¢ºä¿æè¿°åŒ…å«é—œéµå­—ä¸”å¸å¼•äºº",
                        "4. å„²å­˜ä¸¦é‡æ–°éƒ¨ç½²"
                    ]
                }
            ),
            Rule(
                rule_id="R003",
                condition=lambda s: not s.has_jsonld,
                issue={
                    "severity": "high",
                    "title": "ç¼ºå°‘çµæ§‹åŒ–è³‡æ–™ï¼ˆSchema.orgï¼‰",
                    "description": "æ²’æœ‰ JSON-LD çµæ§‹åŒ–è³‡æ–™ï¼ŒAI ç„¡æ³•ç†è§£ä½ çš„ç¶²ç«™é¡å‹èˆ‡å…§å®¹çµæ§‹ã€‚",
                    "why": "çµæ§‹åŒ–è³‡æ–™æ˜¯ AI ç†è§£ç¶²ç«™çš„é—œéµ"
                },
                suggestion={
                    "priority": "high",
                    "effort": "medium",
                    "impact": "high",
                    "action": "æ–°å¢ Schema.org JSON-LD çµæ§‹åŒ–è³‡æ–™",
                    "impact_desc": "å¤§å¹…æå‡ AI å°ç¶²ç«™çš„ç†è§£èˆ‡å¼•ç”¨ç‡",
                    "how_to": [
                        "1. å‰å¾€ https://schema.org é¸æ“‡é©åˆçš„é¡å‹ï¼ˆOrganization/WebSite/Articleï¼‰",
                        "2. ä½¿ç”¨ Google çš„çµæ§‹åŒ–è³‡æ–™æ¨™è¨˜å”åŠ©å·¥å…·ç”¢ç”Ÿ JSON-LD",
                        "3. å°‡ JSON-LD åŠ å…¥ <head> æˆ– <body> å€å¡Š",
                        "4. ä½¿ç”¨ Google Rich Results Test é©—è­‰"
                    ]
                }
            ),
            Rule(
                rule_id="R004",
                condition=lambda s: not s.has_author,
                issue={
                    "severity": "medium",
                    "title": "ç¼ºå°‘ä½œè€…è³‡è¨Š",
                    "description": "å…§å®¹æ²’æœ‰æ˜ç¢ºçš„ä½œè€…ï¼Œé™ä½å¯ä¿¡åº¦èˆ‡æ¬Šå¨æ€§ã€‚",
                    "why": "ä½œè€…è³‡è¨Šæ˜¯å…§å®¹å¯ä¿¡åº¦çš„é‡è¦æŒ‡æ¨™"
                },
                suggestion={
                    "priority": "medium",
                    "effort": "easy",
                    "impact": "medium",
                    "action": "åœ¨å…§å®¹ä¸­åŠ å…¥ä½œè€…è³‡è¨Š",
                    "impact_desc": "æå‡å…§å®¹å¯ä¿¡åº¦èˆ‡å°ˆæ¥­å½¢è±¡",
                    "how_to": [
                        "1. åœ¨æ–‡ç« é é¢åŠ å…¥ä½œè€…åç¨±",
                        "2. ä½¿ç”¨ <meta name='author' content='ä½œè€…åç¨±'>",
                        "3. æˆ–åœ¨ Schema.org Article ä¸­åŠ å…¥ author æ¬„ä½",
                        "4. è€ƒæ…®åŠ å…¥ä½œè€…ç°¡ä»‹èˆ‡ç¤¾ç¾¤é€£çµ"
                    ]
                }
            ),
            Rule(
                rule_id="R005",
                condition=lambda s: not s.has_https,
                issue={
                    "severity": "high",
                    "title": "æœªä½¿ç”¨ HTTPS åŠ å¯†",
                    "description": "ç¶²ç«™ä½¿ç”¨ HTTP è€Œé HTTPSï¼Œå®‰å…¨æ€§ä¸è¶³ã€‚",
                    "why": "HTTPS æ˜¯ç¾ä»£ç¶²ç«™çš„åŸºæœ¬è¦æ±‚"
                },
                suggestion={
                    "priority": "high",
                    "effort": "medium",
                    "impact": "high",
                    "action": "å•Ÿç”¨ HTTPS åŠ å¯†é€£ç·š",
                    "impact_desc": "æå‡å®‰å…¨æ€§èˆ‡æœå°‹å¼•æ“æ’å",
                    "how_to": [
                        "1. å‘ SSL æ†‘è­‰ä¾›æ‡‰å•†ç”³è«‹æ†‘è­‰ï¼ˆLet's Encrypt å…è²»ï¼‰",
                        "2. åœ¨ä¼ºæœå™¨å®‰è£ SSL æ†‘è­‰",
                        "3. è¨­å®š HTTP è‡ªå‹•é‡æ–°å°å‘åˆ° HTTPS",
                        "4. æ›´æ–°æ‰€æœ‰å…§éƒ¨é€£çµç‚º HTTPS"
                    ]
                }
            ),
            # æ–°å¢è¦å‰‡ R006-R008
            Rule(
                rule_id="R006",
                condition=lambda s: not s.has_organization,
                issue={
                    "severity": "medium",
                    "title": "ç¼ºå°‘çµ„ç¹”è³‡è¨Š",
                    "description": "ç¼ºå°‘ Organization schema è®“ AI é›£ä»¥ç¢ºèªç¶²ç«™çš„ç‡Ÿé‹ä¸»é«”ã€‚",
                    "why": "çµ„ç¹”èº«åˆ†é©—è­‰æ˜¯ä¿¡ä»»çš„åŸºçŸ³"
                },
                suggestion={
                    "priority": "medium",
                    "effort": "medium",
                    "impact": "medium",
                    "action": "æ–°å¢ Organization çµæ§‹åŒ–è³‡æ–™",
                    "impact_desc": "å»ºç«‹ç¶²ç«™çš„å¯¦é«”æ¬Šå¨æ„Ÿ"
                }
            ),
            Rule(
                rule_id="R007",
                condition=lambda s: not s.has_social_proof,
                issue={
                    "severity": "low",
                    "title": "ç¤¾ç¾¤è­‰æ˜ä¸è¶³",
                    "description": "ç¶²ç«™æœªé€£çµè¶³å¤ çš„ç¤¾ç¾¤å¹³å°ï¼ˆFacebook, Twitter, LinkedIn ç­‰ï¼‰ã€‚",
                    "why": "ç¤¾ç¾¤é€£çµå¯å¢åŠ ç¶²ç«™çš„ç¤¾æœƒèªå¯"
                },
                suggestion={
                    "priority": "low",
                    "effort": "easy",
                    "impact": "low",
                    "action": "åœ¨é å°¾æˆ–é—œæ–¼é é¢åŠ å…¥ç¤¾ç¾¤åª’é«”é€£çµ",
                    "impact_desc": "æå‡å“ç‰Œå¯å°‹æ€§èˆ‡å¤šç®¡é“ä¿¡ä»»"
                }
            )
        ]

    def extract_signals(self, artifacts: List[Dict[str, Any]]) -> SiteSignals:
        """å¾ artifacts æå–æ‰€æœ‰ä¿¡è™Ÿ (Refactored Logic)"""
        signals = SiteSignals()
        
        scan_artifact = next((a for a in artifacts if a['stage'] == 'scan'), None)
        if not scan_artifact:
            return signals
            
        payload = scan_artifact['jsonb_payload']
        site_url = payload.get('site', '')
        signals.has_https = site_url.startswith('https://')
        
        pages = payload.get('pages', [])
        if not pages:
            return signals
            
        first_page = pages[0]
        
        # 1. åŸºç¤ä¿¡è™Ÿ
        signals.has_title = not first_page.get('title_missing', True)
        signals.has_description = not first_page.get('meta_missing', True)
        signals.has_favicon = first_page.get('has_favicon', False)
        
        # 2. Schema.org æª¢æ¸¬
        self._extract_schema_types(first_page, signals)
        
        # 3. ä½œè€…è³‡è¨Šæå–
        self._extract_author_info(first_page, signals)
        
        # 4. ç¤¾ç¾¤é€£çµæå–
        self._extract_social_links(first_page, signals)
        
        # 5. å¤–éƒ¨é€£çµåˆ†æ
        self._analyze_outbound_links(first_page, signals)
        
        # 6. é é¢èˆ‡è¯ç¹«
        signals.has_about_page = any(p.get('is_about_author', False) for p in pages)
        signals.has_contact = any('contact' in p.get('url', '').lower() for p in pages)
        
        return signals

    def _extract_schema_types(self, page_data: Dict[str, Any], signals: SiteSignals):
        """è§£æ JSON-LD é¡å‹"""
        signals.has_jsonld = page_data.get('has_jsonld', False)
        # æ³¨æ„ï¼šç›®å‰ CLI artifact å¯èƒ½æœªå°‡å®Œæ•´ schemas åˆ—è¡¨å‚³å›ï¼Œ
        # é€™è£¡å‡è¨­æœªä¾†æœƒæœ‰ä¸€å€‹ 'schemas' æ¬„ä½æˆ–ä½¿ç”¨ has_jsonld ä½œç‚ºåŸºç¤
        # æš«æ™‚ä¾è³´å·²æœ‰çš„ flags (å¦‚æœæœ‰)
        # å‡è¨­ CLI æ“´å……å¾Œæœƒæœ‰ schema_types é™£åˆ—
        types = page_data.get('schema_types', [])
        signals.schema_types = types
        signals.schema_count = len(types)
        signals.has_organization = 'Organization' in types
        signals.has_person = 'Person' in types
        signals.has_website = 'WebSite' in types
        signals.has_article = any(t in types for t in ['Article', 'BlogPosting'])

    def _extract_author_info(self, page_data: Dict[str, Any], signals: SiteSignals):
        """æå–ä½œè€…è³‡è¨Š"""
        # å„ªå…ˆä½¿ç”¨ CLI åˆ¤æ–·çš„ is_about_author
        signals.has_author = page_data.get('is_about_author', False)
        # å‡è¨­æœªä¾†æœ‰ä½œè€…åç¨±åˆ—è¡¨
        signals.author_names = page_data.get('authors', [])
        signals.author_count = len(signals.author_names)

    def _extract_social_links(self, page_data: Dict[str, Any], signals: SiteSignals):
        """è­˜åˆ¥ç¤¾ç¾¤é€£çµ"""
        signals.social_links_count = page_data.get('social_links_count', 0)
        signals.has_social_proof = signals.social_links_count >= 2
        # å‡è¨­æœªä¾†æœ‰ platforms åˆ—è¡¨
        signals.social_platforms = page_data.get('social_platforms', [])

    def _analyze_outbound_links(self, page_data: Dict[str, Any], signals: SiteSignals):
        """åˆ†æå¤–éƒ¨é€£çµ"""
        signals.outbound_links_count = page_data.get('external_links_count', 0)
        # å‡è¨­æœªä¾†æœ‰æ¬Šå¨ç¶²åŸŸåˆ—è¡¨
        authorities = page_data.get('authority_domains', [])
        signals.authority_domains = authorities
        signals.has_authority_links = len(authorities) > 0

    def generate_report(self, signals: SiteSignals) -> Dict[str, Any]:
        """ç”¢ç”Ÿå ±å‘Šï¼ˆæ›´æ–°ç‰ˆ - æ”¯æŒç‰¹æ®Šç¶²ç«™æª¢æ¸¬ï¼‰"""
        
        # 0. æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹æ®Šç¶²ç«™
        difficult_site_info = check_difficult_site(signals.url) if hasattr(signals, 'url') else None
        
        # 1. è­˜åˆ¥ç¶²ç«™é¡å‹
        site_type, confidence = classify_site_type(signals)
        
        # 2. è©•ä¼°è¦å‰‡
        fired_rules = []
        issues = []
        suggestions = []
        
        for rule in self.rules:
            if rule.evaluate(signals):
                fired_rules.append(rule.rule_id)
                issues.append(rule.issue)
                suggestions.append(rule.suggestion)
        
        # 3. åŠ å…¥å®¢è£½åŒ–å»ºè­°
        custom_suggestions = generate_custom_suggestions(signals, site_type)
        suggestions.extend(custom_suggestions)
        
        # 4. è¨ˆç®—è©•åˆ†å’Œç¶­åº¦åˆ†æ•¸
        grade, score = self._calculate_grade(signals, len(fired_rules))
        dimensions = calculate_dimension_scores(signals)
        conclusion = self._generate_conclusion(grade, len(fired_rules))
        
        # 5. å¦‚æœæ˜¯ç‰¹æ®Šç¶²ç«™ä¸”è©•åˆ†ç•°å¸¸ä½,æ·»åŠ é ä¼°ä¿¡æ¯
        base_report = {
            "report_version": self.VERSION,
            "signals": signals.model_dump(),
            "score": score,
            "dimensions": dimensions,
            "site_type": site_type,
            "site_type_confidence": confidence,
            "rules_fired": fired_rules,
            "summary": {
                "conclusion": conclusion,
                "grade": grade,
                "score": score,
                "total_issues": len(issues)
            },
            "issues": issues,
            "suggestions": suggestions[:8]
        }
        
        # å¦‚æœæ˜¯ç‰¹æ®Šç¶²ç«™ä¸”æª¢æ¸¬å¤±æ•—(åˆ†æ•¸éä½)
        if difficult_site_info and score < 50:
            estimated_dims = get_estimated_dimensions(difficult_site_info)
            estimated_total = sum(estimated_dims.values())
            estimated_grade = score_to_grade(estimated_total)
            
            # æ§‹å»ºå®Œæ•´çš„ç¶­åº¦çµæ§‹(åŒ…å« max å’Œ items)
            estimated_dimensions_full = {}
            dimension_max_scores = {
                'discoverability': 25,
                'structure': 25,
                'technical': 20,
                'social': 30
            }
            
            for dim_name, dim_score in estimated_dims.items():
                estimated_dimensions_full[dim_name] = {
                    'score': dim_score,
                    'max': dimension_max_scores.get(dim_name, 25),
                    'items': []  # é ä¼°æ¨¡å¼ä¸‹ä¸æä¾›è©³ç´°é …ç›®
                }
            
            base_report.update({
                'is_difficult_site': True,
                'difficult_site_info': difficult_site_info,
                'estimated_score': estimated_total,
                'estimated_grade': estimated_grade,
                'estimated_dimensions': estimated_dimensions_full,
                'detection_status': 'failed',
                'detection_message': f'ç”±æ–¼{difficult_site_info["reason"]},ç„¡æ³•å®Œæ•´æª¢æ¸¬ç¶²ç«™ä¿¡æ¯ã€‚ä»¥ä¸‹ç‚ºåŸºæ–¼ç¶²ç«™é¡å‹çš„é ä¼°åˆ†æ•¸ã€‚'
            })
        else:
            base_report['is_difficult_site'] = False
        
        return base_report
    
    def _calculate_grade(self, signals: SiteSignals, issues_count: int) -> Tuple[str, int]:
        """è¨ˆç®—å¯ä¿¡åº¦ç­‰ç´šï¼ˆä½¿ç”¨åŠ æ¬Šè©•åˆ†ï¼‰"""
        score = calculate_weighted_score(signals)
        grade = score_to_grade(score)
        return grade, score
    
    def _generate_conclusion(self, grade: str, issues_count: int) -> str:
        """ç”¢ç”Ÿä¸€å¥è©±çµè«–"""
        conclusions = {
            "A": "ğŸ‰ æ­å–œï¼ä½ çš„ç¶²ç«™å·²å…·å‚™å„ªç§€çš„ AI å¯ä¿¡åº¦çµæ§‹",
            "B": "âœ… ä½ çš„ç¶²ç«™å·²å…·å‚™åŸºæœ¬å¯ä¿¡çµæ§‹ï¼Œä½†ä»æœ‰æ”¹å–„ç©ºé–“",
            "C": "âš ï¸ ä½ çš„ç¶²ç«™ç¼ºå°‘å¤šé …é—œéµè³‡è¨Šï¼Œå»ºè­°å„ªå…ˆæ”¹å–„",
            "D": "âŒ ç›®å‰ä½ çš„ç¶²ç«™å°šæœªå…·å‚™ç©©å®šçš„ AI å¯å¼•ç”¨å¯ä¿¡åº¦",
            "F": "ğŸš« åˆ†æå¤±æ•—",
            "P": "â³ åˆ†æä¸­..."
        }
        return conclusions.get(grade, "â³ åˆ†æä¸­...")
