# Phase 2ï¼šMachine-Readable Citation System

> **AI Citation Engineering - Phase 2 Implementation**  
> **æ ¸å¿ƒå®šä½**ï¼šCitation as Data, not as Text  
> **è¨­è¨ˆç›®æ¨™**ï¼šè®“å¼•ç”¨æˆç‚ºå¯è¨ˆç®—ã€å¯é©—è­‰ã€å¯æ‹’çµ•çš„ç‰©ä»¶

---

## å®Œæˆåˆ¤æº–ï¼ˆPhase 2 Exit Criteriaï¼‰

âœ… Phase 2 å¿…é ˆæ»¿è¶³ä¸‰å€‹æ¢ä»¶æ‰ç®—å®Œæˆï¼š

1. **ä»»ä½•ä¸€å€‹ AFBï¼Œéƒ½èƒ½è¢«åºåˆ—åŒ–æˆ JSON**
2. **ä»»ä½•ä¸€å€‹ citationï¼Œéƒ½èƒ½è¢«ç¨ç«‹è©•åˆ†**
3. **ç³»çµ±å¯ä»¥æ˜ç¢ºèªªå‡ºï¼šã€Œç‚ºä»€éº¼é€™å€‹ç­”æ¡ˆä¸è©²è¢«ç”¨ã€**

---

## Phase 2-Aï¼šMachine-Readable Citationsï¼ˆç¬¬ä¸€å„ªå…ˆï¼‰

### Citation Object æ ¸å¿ƒå®šç¾©

**è¨­è¨ˆåŸå‰‡**ï¼šå¼•ç”¨ä¸æ˜¯æ–‡å­—ï¼Œæ˜¯å¯è¢«è¨ˆç®—çš„è³‡æ–™ç‰©ä»¶

---

#### Citation Object å¼·åˆ¶çµæ§‹

```json
{
  "@type": "Citation",
  "@context": "https://schema.org",
  
  // å¿…å¡«æ¬„ä½ï¼ˆç¼ºä¸€ä¸å¯ï¼‰
  "citation_id": "cite-2026-001",
  
  "source_entity": {
    "@id": "https://example.com/entity/stanford-ai-lab",
    "name": "Stanford AI Lab",
    "entity_confidence": 0.95
  },
  
  "claim": "AFB increases AI citation likelihood by 3.4x compared to traditional content",
  
  "evidence_type": "peer_reviewed",
  
  "source_url": "https://stanford.edu/research/ai-citation-2025",
  
  "publication_date": "2025-12-01T00:00:00Z",
  
  "verification_status": "verified",
  
  "confidence": 0.92,
  
  // é¸å¡«ä½†å¼·çƒˆå»ºè­°
  "last_verified": "2026-02-06T10:00:00Z",
  "verification_method": "cross_reference",
  "cross_verified_by": [
    "https://example.com/entity/mit-media-lab",
    "https://example.com/entity/berkeley-ai-research"
  ],
  "sample_size": 500,
  "study_duration": "6 months",
  "claim_specificity": "quantitative",
  "limitations": [],
  "data_availability": "public"
}
```

---

**Phase 2 å®Œæ•´å…§å®¹å·²è£œå……è‡³è¦åŠƒæ–‡æª”ç¬¬ 3550-4000 è¡Œå€æ®µã€‚**

**ä¸»è¦ç« ç¯€**ï¼š
1. Citation Object å®šç¾©
2. Evidence Type åˆ†é¡
3. Verification Status ç‹€æ…‹æ©Ÿ
4. Citation â†” AFB ç¶å®šè¦å‰‡
5. Citation Failure States
6. Citation Quality Evaluation
7. Entity Graph.json å¯¦ä½œ

---

---

## Phase 2-Bï¼šCitation Quality Evaluationï¼ˆç¬¬äºŒå„ªå…ˆï¼‰

### æ ¸å¿ƒå®šç¾©ï¼šCitation Confidence Score (CCS)

**é—œéµåŸå‰‡**ï¼šCCS å¿…é ˆç¨ç«‹æ–¼ Entity Confidence (EC)

**ç‚ºä»€éº¼å¿…é ˆç¨ç«‹ï¼Ÿ**
- Entity Confidence (EC)ï¼šé€™å€‹å¯¦é«”æ•´é«”å¯ä¿¡åº¦å¦‚ä½•
- Citation Confidence (CCS)ï¼šé€™ä¸€ç­†è­‰æ“šæœ¬èº«å¯é åº¦å¦‚ä½•

**æ ¸å¿ƒæ´å¯Ÿ**ï¼šåŒä¸€å€‹ entity å¯ä»¥åŒæ™‚æœ‰é«˜ CCS èˆ‡ä½ CCS citationã€‚

> å¦‚æœ CCS èˆ‡ EC æ··åœ¨ä¸€èµ·ï¼Œç³»çµ±æœƒè¢«æ¬Šå¨ç¶æ¶ã€‚

---

### CCS è¨ˆç®—ç¶­åº¦ï¼ˆ6 ç¶­æ¨¡å‹ï¼‰

```python
CCS_DIMENSIONS = {
    "Evidence_Strength": {
        "weight": 0.20,
        "symbol": "E",
        "description": "è­‰æ“šé¡å‹å¼·åº¦"
    },
    "Source_Reputation": {
        "weight": 0.18,
        "symbol": "R",
        "description": "ä¾†æºä¿¡è­½ï¼ˆæ©Ÿæ§‹/å‡ºç‰ˆç‰©å±¤ç´šï¼‰"
    },
    "Corroboration": {
        "weight": 0.28,  # æœ€é«˜æ¬Šé‡
        "symbol": "C",
        "description": "è·¨ä¾†æºäº¤å‰é©—è­‰å¼·åº¦"
    },
    "Recency_Decay": {
        "weight": 0.14,
        "symbol": "T",
        "description": "æ™‚æ•ˆè¡°æ¸›"
    },
    "Claim_Specificity": {
        "weight": 0.12,
        "symbol": "S",
        "description": "ä¸»å¼µå¯é©—è­‰ç¨‹åº¦"
    },
    "Verification_Status": {
        "weight": 0.08,
        "symbol": "V",
        "description": "é©—è­‰ç‹€æ…‹æ©Ÿçµæœ"
    }
}
```

**ç‚ºä»€éº¼ Corroboration (C) æœ€å¤§ï¼Ÿ**

> ç”Ÿæˆå¼•æ“æœ€æ€•å–®ä¸€ä¾†æºå¤±èª¤ï¼›è·¨ä¾†æºä¸€è‡´æ˜¯ã€Œå®‰å…¨æ„Ÿã€ã€‚

---

### CCS è¨ˆç®—å…¬å¼ (v1)

```python
def calculate_citation_confidence_score(citation):
    """
    è¨ˆç®— Citation Confidence Score (CCS)
    
    æ¯å€‹ç¶­åº¦æ¨™æº–åŒ–åˆ° 0~1
    """
    E = calculate_evidence_strength(citation.evidence_type)
    R = calculate_source_reputation(citation.source_entity)
    C = calculate_corroboration(citation)
    T = calculate_recency_decay(citation.publication_date, citation.topic_type)
    S = calculate_claim_specificity(citation.claim)
    V = calculate_verification_score(citation.verification_status)
    
    CCS = (
        0.28 * C +  # Corroborationï¼ˆæœ€é‡è¦ï¼‰
        0.20 * E +  # Evidence Strength
        0.18 * R +  # Source Reputation
        0.14 * T +  # Recency Decay
        0.12 * S +  # Claim Specificity
        0.08 * V    # Verification Status
    )
    
    return {
        "ccs": CCS,
        "breakdown": {
            "corroboration": C,
            "evidence_strength": E,
            "source_reputation": R,
            "recency": T,
            "specificity": S,
            "verification": V
        }
    }
```

---

### ç¶­åº¦ 1ï¼šEvidence Strength (E)

**æ˜ å°„è¡¨ï¼ˆå›ºå®š baseï¼Œå¯èª¿ï¼‰**ï¼š

```python
EVIDENCE_STRENGTH_MAP = {
    "peer_reviewed": 1.00,
    "institutional_research": 0.90,
    "standards_specification": 0.85,
    "first_party_data_with_method": 0.80,
    "verified_case_study": 0.75,
    "reputable_media": 0.70,
    "expert_opinion": 0.60,
    "industry_report": 0.55,
    "community_consensus": 0.45,
    "documented_test": 0.40,
    "secondary_source": 0.30,
    "anecdotal": 0.15,
    "anonymous_or_unverified": 0.10
}

def calculate_evidence_strength(evidence_type):
    """è¨ˆç®—è­‰æ“šå¼·åº¦"""
    return EVIDENCE_STRENGTH_MAP.get(evidence_type, 0.10)
```

---

### ç¶­åº¦ 2ï¼šSource Reputation (R)

```python
def calculate_source_reputation(source_entity):
    """
    è¨ˆç®—ä¾†æºä¿¡è­½
    çµåˆ Entity Confidence èˆ‡æ©Ÿæ§‹é¡å‹
    """
    entity_conf = source_entity.entity_confidence
    
    # æ©Ÿæ§‹é¡å‹åŠ æˆ
    institution_type = source_entity.institution_type
    type_bonus = {
        "academic": 0.15,
        "government": 0.12,
        "standards_body": 0.10,
        "major_media": 0.08,
        "tech_company": 0.05,
        "individual": 0.00
    }.get(institution_type, 0.0)
    
    # ç¶œåˆè¨ˆç®—
    reputation = min(entity_conf + type_bonus, 1.0)
    
    return reputation
```

---

### ç¶­åº¦ 3ï¼šCorroboration (C) - æœ€é—œéµ

```python
def calculate_corroboration(citation):
    """
    è¨ˆç®—è·¨ä¾†æºé©—è­‰å¼·åº¦
    é€™æ˜¯ CCS æœ€é‡è¦çš„ç¶­åº¦
    """
    # åŸºç¤ï¼šæœ‰å¤šå°‘ç¨ç«‹ä¾†æºæ”¯æŒç›¸åŒ claim
    supporting_citations = find_supporting_citations(citation.claim)
    distinct_sources = count_distinct_sources(supporting_citations)
    
    # ä¾†æºå¤šæ¨£æ€§
    if distinct_sources == 0:
        base_score = 0.0
    elif distinct_sources == 1:
        base_score = 0.3  # å–®ä¸€ä¾†æºï¼Œä½åˆ†
    elif distinct_sources == 2:
        base_score = 0.6
    elif distinct_sources == 3:
        base_score = 0.8
    else:  # â‰¥ 4
        base_score = 1.0
    
    # ä¾†æºå“è³ªåŠ æ¬Š
    source_quality_avg = sum([
        c.source_entity.entity_confidence 
        for c in supporting_citations
    ]) / len(supporting_citations) if supporting_citations else 0.0
    
    # ç¶œåˆåˆ†æ•¸
    corroboration = (base_score * 0.7) + (source_quality_avg * 0.3)
    
    return corroboration
```

---

### ç¶­åº¦ 4ï¼šRecency Decay (T) - æ™‚æ•ˆè¡°æ¸›

**åŠè¡°æœŸæ¨¡å‹**ï¼š

```python
import math
from datetime import datetime, timedelta

HALF_LIFE_DAYS = {
    "fast_moving": 90,    # AI å·¥å…·ã€æ”¿ç­–ã€åƒ¹æ ¼
    "medium": 180,        # è¡ŒéŠ·ç­–ç•¥ã€å¹³å°æ©Ÿåˆ¶
    "slow": 720,          # æ•¸å­¸å®šç¾©ã€é•·æœŸåŸç†
    "fundamental": None   # ä¸è¡°æ¸›
}

def calculate_recency_decay(publication_date, topic_type):
    """
    è¨ˆç®—æ™‚æ•ˆè¡°æ¸›åˆ†æ•¸
    ä½¿ç”¨åŠè¡°æœŸæ¨¡å‹
    """
    half_life = HALF_LIFE_DAYS.get(topic_type, 180)
    
    if half_life is None:
        return 1.0  # åŸºç¤æ¦‚å¿µä¸è¡°æ¸›
    
    age_days = (datetime.now() - publication_date).days
    
    # åŠè¡°æœŸå…¬å¼ï¼šT = 0.5^(age_days / half_life_days)
    decay_score = 0.5 ** (age_days / half_life)
    
    return max(decay_score, 0.1)  # æœ€ä½ä¿ç•™ 0.1
```

**ç¯„ä¾‹**ï¼š
```python
# AI å·¥å…·è©•æ¸¬ï¼ˆfast_movingï¼ŒåŠè¡°æœŸ 90 å¤©ï¼‰
publication_date = datetime(2025, 8, 1)
current_date = datetime(2026, 2, 6)
age_days = 189

T = 0.5 ** (189 / 90) = 0.24  # å·²é¡¯è‘—è¡°æ¸›

# æ•¸å­¸åŸç†ï¼ˆfundamentalï¼Œä¸è¡°æ¸›ï¼‰
T = 1.0  # ä¿æŒå®Œæ•´
```

---

### ç¶­åº¦ 5ï¼šClaim Specificity (S) - å¯é©—è­‰æ€§

**å¯è¨ˆç®—ç‰ˆæœ¬ï¼ˆ0~1ï¼‰**ï¼š

```python
def calculate_claim_specificity(claim):
    """
    è¨ˆç®—ä¸»å¼µçš„å¯é©—è­‰ç¨‹åº¦
    åŸºæ–¼ã€Œå¯é©—è­‰è¦ç´ ã€è¨ˆåˆ†
    """
    score = 0.0
    
    # +0.25ï¼šæœ‰å…·é«”æ•¸å­—
    if has_quantitative_data(claim):
        score += 0.25
        # ç¯„ä¾‹ï¼šã€Œæå‡ 340%ã€ã€ã€ŒN=500ã€
    
    # +0.25ï¼šæœ‰æ˜ç¢ºæ–¹æ³•
    if has_methodology(claim):
        score += 0.25
        # ç¯„ä¾‹ï¼šã€ŒA/B testã€ã€ã€Œrandomized trialã€
    
    # +0.25ï¼šæœ‰æ™‚é–“ç¯„åœ
    if has_timeframe(claim):
        score += 0.25
        # ç¯„ä¾‹ï¼šã€Œ2025-01 è‡³ 2025-06ã€
    
    # +0.25ï¼šæœ‰é‚Šç•Œæ¢ä»¶
    if has_boundary_conditions(claim):
        score += 0.25
        # ç¯„ä¾‹ï¼šã€Œé©ç”¨æ–¼ Entity Confidence > 0.70 çš„ç¶²ç«™ã€
    
    return score

# è¼”åŠ©å‡½æ•¸
def has_quantitative_data(claim):
    """æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—ã€ç™¾åˆ†æ¯”ã€æ¨£æœ¬æ•¸"""
    import re
    patterns = [
        r'\d+%',           # ç™¾åˆ†æ¯”
        r'\d+x',           # å€æ•¸
        r'N\s*=\s*\d+',    # æ¨£æœ¬æ•¸
        r'\d+\.\d+',       # å°æ•¸
    ]
    return any(re.search(p, claim) for p in patterns)

def has_methodology(claim):
    """æª¢æŸ¥æ˜¯å¦èªªæ˜ç ”ç©¶æ–¹æ³•"""
    methods = [
        'A/B test', 'survey', 'experiment', 'trial',
        'study', 'analysis', 'measurement', 'testing'
    ]
    return any(m.lower() in claim.lower() for m in methods)
```

**ç¯„ä¾‹**ï¼š
```python
# é«˜ Specificity ç¯„ä¾‹
claim = "Based on A/B testing with N=500 websites over 6 months (Jan-Jun 2025), Entity-first structure increased AI citation rate by 340% for sites with Entity Confidence > 0.70."

S = 1.0  # å››å€‹è¦ç´ å…¨æ»¿

# ä½ Specificity ç¯„ä¾‹
claim = "AI-Ready SEO improves results significantly."

S = 0.0  # ç„¡å…·é«”å¯é©—è­‰è¦ç´ 
```

---

### ç¶­åº¦ 6ï¼šVerification Status (V)

```python
VERIFICATION_STATUS_SCORE = {
    "verified": 1.0,
    "pending": 0.5,
    "unverifiable": 0.1,
    "outdated": 0.3,
    "contradicted": 0.0,
    "retracted": 0.0,
    "disputed": 0.4
}

def calculate_verification_score(verification_status):
    """è¨ˆç®—é©—è­‰ç‹€æ…‹åˆ†æ•¸"""
    return VERIFICATION_STATUS_SCORE.get(verification_status, 0.1)
```

---

### Citation ç«¶çˆ­èˆ‡è¡çªè™•ç†ï¼ˆé¡¯å¼è¦å‰‡ï¼‰

#### è¦å‰‡ 1ï¼šåŒä¸€ Claimï¼Œå¤šç­† Citation â†’ Top-K + èšåˆ

```python
def handle_multiple_citations_same_claim(citations, K=3):
    """
    è™•ç†å¤šå€‹æ”¯æŒç›¸åŒ claim çš„ citations
    """
    # æŒ‰ CCS æ’åº
    sorted_citations = sorted(
        citations, 
        key=lambda c: c.ccs, 
        reverse=True
    )
    
    # å–å‰ K å€‹
    top_k = sorted_citations[:K]
    
    # æª¢æŸ¥ä¾†æºå¤šæ¨£æ€§
    distinct_sources = len(set([c.source_entity.id for c in top_k]))
    
    # è¨ˆç®—èšåˆä¿¡å¿ƒ
    if distinct_sources >= 2:
        diversity_bonus = 0.1
    else:
        diversity_bonus = 0.0
    
    aggregated_confidence = (
        sum([c.ccs for c in top_k]) / K +
        diversity_bonus
    )
    
    return {
        "selected_citations": top_k,
        "aggregated_confidence": min(aggregated_confidence, 1.0),
        "distinct_sources": distinct_sources,
        "diversity_bonus_applied": diversity_bonus > 0
    }
```

---

#### è¦å‰‡ 2ï¼šåŒä¸€ Claimï¼Œçµè«–ç›¸å â†’ é™ç´šæˆ–æ‹’ç”¨

**Conflict åˆ¤æ–·**ï¼š

```python
def detect_citation_conflict(citation_a, citation_b):
    """
    åˆ¤æ–·å…©å€‹ citation æ˜¯å¦è¡çª
    """
    # æª¢æŸ¥ 1ï¼šä¸»é¡Œæ˜¯å¦ç›¸åŒ
    if not is_same_topic(citation_a.claim, citation_b.claim):
        return False
    
    # æª¢æŸ¥ 2ï¼šçµè«–æ–¹å‘æ˜¯å¦ç›¸å
    direction_a = extract_claim_direction(citation_a.claim)
    direction_b = extract_claim_direction(citation_b.claim)
    
    if direction_a != direction_b:
        return True
    
    # æª¢æŸ¥ 3ï¼šæ•¸å€¼å·®ç•°æ˜¯å¦è¶…éé–¾å€¼
    value_a = extract_quantitative_value(citation_a.claim)
    value_b = extract_quantitative_value(citation_b.claim)
    
    if value_a and value_b:
        diff_ratio = abs(value_a - value_b) / max(value_a, value_b)
        if diff_ratio > 0.20:  # 20% é–¾å€¼
            return True
    
    return False
```

**è™•ç†ç­–ç•¥ï¼ˆä¸‰æ®µå¼ï¼‰**ï¼š

```python
def resolve_citation_conflict(high_ccs_citation, low_ccs_citation):
    """
    è§£æ±º citation è¡çª
    """
    ccs_gap = high_ccs_citation.ccs - low_ccs_citation.ccs
    
    # æƒ…å¢ƒ 1ï¼šé«˜ CCS æ–¹æ˜é¡¯é ˜å…ˆï¼ˆâ‰¥ 0.20ï¼‰
    if ccs_gap >= 0.20:
        return {
            "action": "USE_HIGH_WITH_WARNING",
            "selected": high_ccs_citation,
            "afb_status": "contested",
            "warning": f"Conflicting evidence exists with lower confidence (CCS: {low_ccs_citation.ccs:.2f})"
        }
    
    # æƒ…å¢ƒ 2ï¼šå·®è·å°ï¼ˆ< 0.20ï¼‰
    elif ccs_gap < 0.20:
        return {
            "action": "DOWNGRADE_AFB",
            "selected": None,
            "afb_status": "low_confidence",
            "warning": "Conflicting evidence with similar confidence levels",
            "recommendation": "Require additional sources before use"
        }
    
    # æƒ…å¢ƒ 3ï¼šä»»ä¸€ç‚º failure state
    if (high_ccs_citation.verification_status in ["unverifiable", "retracted"] or
        low_ccs_citation.verification_status in ["unverifiable", "retracted"]):
        return {
            "action": "REJECT",
            "selected": None,
            "afb_status": "rejected",
            "reason": "One or more citations in failure state"
        }
```

---

#### è¦å‰‡ 3ï¼šé«˜æ¬Šå¨ä½†éæœŸ vs ä½æ¬Šå¨ä½†æ–° â†’ Trade-off

**å®‰å…¨åå¥½ç­–ç•¥**ï¼š

```python
def handle_authority_vs_recency_tradeoff(old_citation, new_citation):
    """
    è™•ç†æ¬Šå¨æ€§èˆ‡æ™‚æ•ˆæ€§çš„æ¬Šè¡¡
    """
    old_T = old_citation.ccs_breakdown['recency']
    new_T = new_citation.ccs_breakdown['recency']
    
    old_R = old_citation.ccs_breakdown['source_reputation']
    new_R = new_citation.ccs_breakdown['source_reputation']
    
    # æƒ…å¢ƒ 1ï¼šèˆŠ citation æ˜é¡¯éæœŸï¼ˆT < 0.35ï¼‰
    if old_T < 0.35:
        # å¿…é ˆæœ‰è·¨ä¾†æºé©—è­‰æ‰èƒ½ç”¨
        old_C = old_citation.ccs_breakdown['corroboration']
        
        if old_C >= 0.5:
            return {
                "action": "USE_OLD_WITH_CORROBORATION",
                "selected": old_citation,
                "warning": "Aged citation, but cross-verified"
            }
        else:
            return {
                "action": "PREFER_NEW",
                "selected": new_citation,
                "reason": "Old citation lacks corroboration and significantly aged"
            }
    
    # æƒ…å¢ƒ 2ï¼šæ–° citation å¯é©—è­‰ä½†æ¬Šå¨è¼ƒä½
    if new_citation.verification_status == "verified" and new_R < 0.60:
        return {
            "action": "USE_NEW_BUT_FLAG",
            "selected": new_citation,
            "warning": "Recent but from lower-authority source"
        }
    
    # é è¨­ï¼šæ¬Šè¡¡ CCS ç¸½åˆ†
    if old_citation.ccs > new_citation.ccs:
        return {
            "action": "USE_OLD",
            "selected": old_citation
        }
    else:
        return {
            "action": "USE_NEW",
            "selected": new_citation
        }
```

---

### CCS å®Œæ•´å¯¦ä½œç¯„ä¾‹

```python
# ç¯„ä¾‹ Citation
citation = {
    "citation_id": "cite-stanford-2025-001",
    "claim": "Based on A/B testing with N=500 websites over 6 months, Entity-first structure increased AI citation rate by 340%.",
    "evidence_type": "peer_reviewed",
    "source_entity": {
        "id": "stanford-ai-lab",
        "entity_confidence": 0.95,
        "institution_type": "academic"
    },
    "publication_date": datetime(2025, 12, 1),
    "topic_type": "medium",
    "verification_status": "verified",
    "cross_verified_by": [
        "mit-media-lab",
        "berkeley-ai-research"
    ]
}

# è¨ˆç®— CCS
E = 1.00  # peer_reviewed
R = min(0.95 + 0.15, 1.0) = 1.00  # academic + high EC
C = 0.80  # 3 distinct sources
T = 0.5 ** (67/180) = 0.77  # 67 days old, medium topic
S = 1.00  # æœ‰æ•¸å­—ã€æ–¹æ³•ã€æ™‚é–“ã€é‚Šç•Œ
V = 1.00  # verified

CCS = 0.28*0.80 + 0.20*1.00 + 0.18*1.00 + 0.14*0.77 + 0.12*1.00 + 0.08*1.00
    = 0.224 + 0.200 + 0.180 + 0.108 + 0.120 + 0.080
    = 0.912

# è©•ç´šï¼šğŸŒŸ å„ªç§€ï¼ˆâ‰¥ 0.90ï¼‰
```

---

---

## Phase 2-Cï¼šEntity Graph.jsonï¼ˆç¬¬ä¸‰å„ªå…ˆï¼‰

### Graph çš„ç›®çš„ï¼ˆåªåšé¢¨éšªæª¢æ¸¬ï¼‰

**æ˜ç¢ºå®šä½**ï¼šä¸æ˜¯è¦–è¦ºåŒ–ï¼Œä¸æ˜¯çŸ¥è­˜åœ–è­œç‚«æŠ€ã€‚

**åªåšä¸‰ä»¶äº‹**ï¼š
1. **Isolated Answer æª¢æ¸¬**ï¼šç­”æ¡ˆæ˜¯å¦å­¤ç«‹
2. **Single-Source Risk æª¢æ¸¬**ï¼šæ˜¯å¦å–®ä¸€ä¾†æºä¾è³´
3. **Phase 3 æ¯”å°åŸºæº–**ï¼šåå‘ GEO éœ€è¦

---

### æœ€å° Graph Schema (v1)

#### Node é¡å‹ï¼ˆ4 ç¨®ï¼‰

```python
NODE_TYPES = {
    "entity": {
        "description": "å…§å®¹å¯¦é«”ï¼ˆPerson / Organization / Conceptï¼‰",
        "required_fields": ["id", "type", "label", "entity_confidence"]
    },
    "afb": {
        "description": "Answer-First Block",
        "required_fields": ["id", "type", "label", "entity_id"]
    },
    "citation": {
        "description": "å¼•ç”¨è­‰æ“š",
        "required_fields": ["id", "type", "label", "ccs"]
    },
    "source": {
        "description": "ä¾†æºæ©Ÿæ§‹/å¹³å°",
        "required_fields": ["id", "type", "label", "authority"]
    }
}
```

#### Edge é¡å‹ï¼ˆ3 ç¨®ï¼‰

```python
EDGE_TYPES = {
    "answers": {
        "from": "entity",
        "to": "afb",
        "description": "Entity æä¾› AFB"
    },
    "supported_by": {
        "from": "afb",
        "to": "citation",
        "description": "AFB ç”± Citation æ”¯æŒ"
    },
    "from_source": {
        "from": "citation",
        "to": "source",
        "description": "Citation ä¾†è‡ª Source"
    }
}
```

---

### è¼¸å‡º JSON æ ¼å¼ï¼ˆv1 ç›´æ¥å¯ç”¨ï¼‰

```json
{
  "graph_version": "2.0",
  "generated_at": "2026-02-06T10:00:00+08:00",
  "system_metadata": {
    "total_entities": 2,
    "total_afbs": 4,
    "total_citations": 9,
    "total_sources": 3,
    "isolated_afbs": 0,
    "high_risk_afbs": 1
  },
  
  "nodes": [
    {
      "id": "ent:ai-citation-engineering",
      "type": "entity",
      "label": "AI Citation Engineering",
      "entity_confidence": 0.87,
      "afb_count": 4,
      "citation_count": 9
    },
    {
      "id": "afb:definition:v1",
      "type": "afb",
      "label": "Definition AFB",
      "entity_id": "ent:ai-citation-engineering",
      "confidence": 0.89,
      "citation_count": 3
    },
    {
      "id": "cite:2026-001",
      "type": "citation",
      "label": "Stanford study 2025",
      "ccs": 0.91,
      "evidence_type": "peer_reviewed",
      "verification_status": "verified"
    },
    {
      "id": "src:stanford",
      "type": "source",
      "label": "Stanford AI Lab",
      "authority": 0.98,
      "institution_type": "academic"
    }
  ],
  
  "edges": [
    {
      "from": "ent:ai-citation-engineering",
      "to": "afb:definition:v1",
      "type": "answers"
    },
    {
      "from": "afb:definition:v1",
      "to": "cite:2026-001",
      "type": "supported_by",
      "weight": 0.91
    },
    {
      "from": "cite:2026-001",
      "to": "src:stanford",
      "type": "from_source"
    }
  ],
  
  "metrics": {
    "ent:ai-citation-engineering": {
      "afbs": 4,
      "citations": 9,
      "distinct_sources": 3,
      "avg_ccs": 0.78,
      "is_isolated": false,
      "single_source_risk": false,
      "lowest_ccs": 0.62,
      "highest_ccs": 0.91,
      "conflict_count": 1,
      "verified_citation_ratio": 0.89
    },
    "afb:definition:v1": {
      "citations": 3,
      "distinct_sources": 3,
      "is_isolated": false,
      "single_source_risk": false,
      "avg_ccs": 0.86,
      "risk_level": "low"
    }
  }
}
```

---

### å­¤ç«‹ç¯€é»èˆ‡é¢¨éšªè¦å‰‡ï¼ˆå¯«æ­»è¦å‰‡ï¼‰

#### Isolated Answer å®šç¾©

```python
def check_isolated_answer(afb, graph):
    """
    åˆ¤æ–· AFB æ˜¯å¦ç‚ºå­¤ç«‹ç­”æ¡ˆ
    """
    # æ¢ä»¶ 1ï¼šæ²’æœ‰ä»»ä½• citation
    citations = graph.get_citations_for_afb(afb.id)
    if len(citations) == 0:
        return {
            "is_isolated": True,
            "reason": "No citations",
            "severity": "critical"
        }
    
    # æ¢ä»¶ 2ï¼šæ‰€æœ‰ citations éƒ½è™•æ–¼ failure state
    all_failed = all(
        c.verification_status in ["unverifiable", "retracted", "contradicted"]
        for c in citations
    )
    if all_failed:
        return {
            "is_isolated": True,
            "reason": "All citations in failure state",
            "severity": "critical"
        }
    
    # æ¢ä»¶ 3ï¼šdistinct_sources < 2 ä¸” claim æ¶‰åŠæ•¸å­—/ç ”ç©¶
    distinct_sources = count_distinct_sources(citations)
    claim_type = classify_claim_type(afb.claim)
    
    if (distinct_sources < 2 and 
        claim_type in ["statistical", "research", "comparative"]):
        return {
            "is_isolated": True,
            "reason": "Single source for quantitative claim",
            "severity": "high"
        }
    
    return {
        "is_isolated": False,
        "severity": "none"
    }
```

---

#### Single-Source Risk å®šç¾©

```python
def check_single_source_risk(afb, graph):
    """
    åˆ¤æ–· AFB æ˜¯å¦æœ‰å–®ä¸€ä¾†æºé¢¨éšª
    """
    citations = graph.get_citations_for_afb(afb.id)
    distinct_sources = count_distinct_sources(citations)
    claim_type = classify_claim_type(afb.claim)
    
    # è¦å‰‡ï¼šdistinct_sources = 1 ä¸” claim ç‚º stat/causal/comparative
    if (distinct_sources == 1 and 
        claim_type in ["statistical", "causal", "comparative"]):
        return {
            "single_source_risk": True,
            "risk_level": "high",
            "source_id": citations[0].source_entity.id,
            "recommendation": "Require at least one additional independent source"
        }
    
    # ä½é¢¨éšªä½†æ¨™è¨˜
    if distinct_sources == 1:
        return {
            "single_source_risk": True,
            "risk_level": "medium",
            "source_id": citations[0].source_entity.id,
            "recommendation": "Consider adding corroborating source"
        }
    
    return {
        "single_source_risk": False,
        "risk_level": "low"
    }
```

---

### Graph é¢¨éšªè©•ä¼°å®Œæ•´å¯¦ä½œ

```python
class EntityGraph:
    """Entity Graph é¢¨éšªè©•ä¼°ç³»çµ±"""
    
    def __init__(self):
        self.nodes = []
        self.edges = []
        self.metrics = {}
    
    def calculate_entity_metrics(self, entity_id):
        """è¨ˆç®— Entity å±¤ç´šçš„é¢¨éšªæŒ‡æ¨™"""
        afbs = self.get_afbs_for_entity(entity_id)
        all_citations = []
        
        for afb in afbs:
            citations = self.get_citations_for_afb(afb.id)
            all_citations.extend(citations)
        
        # ç¨ç«‹ä¾†æºæ•¸
        distinct_sources = count_distinct_sources(all_citations)
        
        # CCS çµ±è¨ˆ
        ccs_scores = [c.ccs for c in all_citations if c.ccs]
        avg_ccs = sum(ccs_scores) / len(ccs_scores) if ccs_scores else 0.0
        lowest_ccs = min(ccs_scores) if ccs_scores else 0.0
        highest_ccs = max(ccs_scores) if ccs_scores else 0.0
        
        # é©—è­‰ç‡
        verified_count = sum(
            1 for c in all_citations 
            if c.verification_status == "verified"
        )
        verified_ratio = verified_count / len(all_citations) if all_citations else 0.0
        
        # è¡çªæ•¸
        conflict_count = count_citation_conflicts(all_citations)
        
        # å­¤ç«‹èˆ‡é¢¨éšªæª¢æ¸¬
        isolated_afbs = []
        single_source_afbs = []
        
        for afb in afbs:
            isolated_check = check_isolated_answer(afb, self)
            if isolated_check['is_isolated']:
                isolated_afbs.append(afb.id)
            
            risk_check = check_single_source_risk(afb, self)
            if risk_check['single_source_risk']:
                single_source_afbs.append(afb.id)
        
        return {
            "entity_id": entity_id,
            "afbs": len(afbs),
            "citations": len(all_citations),
            "distinct_sources": distinct_sources,
            "avg_ccs": round(avg_ccs, 3),
            "lowest_ccs": round(lowest_ccs, 3),
            "highest_ccs": round(highest_ccs, 3),
            "verified_citation_ratio": round(verified_ratio, 3),
            "conflict_count": conflict_count,
            "is_isolated": len(isolated_afbs) > 0,
            "isolated_afbs": isolated_afbs,
            "single_source_risk": len(single_source_afbs) > 0,
            "single_source_afbs": single_source_afbs,
            "risk_assessment": self.assess_overall_risk(
                distinct_sources,
                lowest_ccs,
                verified_ratio,
                len(isolated_afbs),
                len(single_source_afbs)
            )
        }
    
    def assess_overall_risk(self, distinct_sources, lowest_ccs, 
                           verified_ratio, isolated_count, single_source_count):
        """ç¶œåˆé¢¨éšªè©•ä¼°"""
        risk_score = 0
        
        # é¢¨éšªå› å­
        if distinct_sources < 2:
            risk_score += 3
        if lowest_ccs < 0.60:
            risk_score += 2
        if verified_ratio < 0.70:
            risk_score += 2
        if isolated_count > 0:
            risk_score += 4
        if single_source_count > 0:
            risk_score += 3
        
        # è©•ç´š
        if risk_score >= 8:
            return {"level": "critical", "action": "REJECT"}
        elif risk_score >= 5:
            return {"level": "high", "action": "DOWNGRADE"}
        elif risk_score >= 3:
            return {"level": "medium", "action": "FLAG"}
        else:
            return {"level": "low", "action": "ACCEPT"}
    
    def export_json(self, output_file="entity_graph.json"):
        """åŒ¯å‡º Graph JSON"""
        graph_data = {
            "graph_version": "2.0",
            "generated_at": datetime.now().isoformat(),
            "system_metadata": self.calculate_system_metadata(),
            "nodes": self.nodes,
            "edges": self.edges,
            "metrics": self.metrics
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(graph_data, f, indent=2, ensure_ascii=False)
        
        return output_file
```

---

### Graph ä½¿ç”¨ç¯„ä¾‹

#### ç¯„ä¾‹ 1ï¼šå¥åº·çš„ Entity Graph

```json
{
  "metrics": {
    "ent:ai-citation-engineering": {
      "afbs": 4,
      "citations": 9,
      "distinct_sources": 5,
      "avg_ccs": 0.82,
      "lowest_ccs": 0.68,
      "verified_citation_ratio": 0.89,
      "is_isolated": false,
      "single_source_risk": false,
      "risk_assessment": {
        "level": "low",
        "action": "ACCEPT"
      }
    }
  }
}
```

#### ç¯„ä¾‹ 2ï¼šé«˜é¢¨éšª Entity Graph

```json
{
  "metrics": {
    "ent:new-concept": {
      "afbs": 2,
      "citations": 2,
      "distinct_sources": 1,  // å–®ä¸€ä¾†æº
      "avg_ccs": 0.55,  // ä½ CCS
      "lowest_ccs": 0.45,
      "verified_citation_ratio": 0.50,  // ä½é©—è­‰ç‡
      "is_isolated": true,  // æœ‰å­¤ç«‹ AFB
      "isolated_afbs": ["afb:new-001"],
      "single_source_risk": true,
      "single_source_afbs": ["afb:new-001", "afb:new-002"],
      "risk_assessment": {
        "level": "critical",
        "action": "REJECT",
        "reason": "Multiple risk factors: isolated AFBs, single source, low CCS"
      }
    }
  }
}
```

---

## Phase 2 å®Œæˆé©—æ”¶ï¼šGo / No-Go æª¢æŸ¥æ¸…å–®

### âœ… Goï¼ˆPhase 2 å®Œæˆï¼‰

Phase 2 å¿…é ˆæ»¿è¶³ä»¥ä¸‹æ‰€æœ‰æ¢ä»¶ï¼š

- [x] **AFB JSON å¯è¼¸å‡º**ï¼šæ¯å€‹ AFB éƒ½èƒ½åˆ—å‡ºå®Œæ•´å¼•ç”¨æ¸…å–®
- [x] **Citation å¯ç¨ç«‹è©•åˆ†**ï¼šæ¯ç­† citation éƒ½èƒ½ç®—å‡º CCSï¼ˆ0~1ï¼‰
- [x] **Conflict åˆ¤å®šèˆ‡è™•ç†**ï¼šæœ‰æ˜ç¢ºçš„ accept / downgrade / reject æ±ºç­–
- [x] **Graph.json å¯ç”Ÿæˆ**ï¼šèƒ½ç®—å‡ºæ¯å€‹ entity çš„ distinct_sources / isolated / risk
- [x] **æ‹’çµ•ç†ç”±å¯è¼¸å‡º**ï¼šç³»çµ±èƒ½èªªæ˜ç‚ºä»€éº¼æ‹’çµ•ï¼ˆfailure state æˆ– conflictï¼‰
- [x] **CCS ç¨ç«‹æ–¼ EC**ï¼šCitation Confidence ä¸å— Entity Confidence ç¶æ¶
- [x] **Corroboration å„ªå…ˆ**ï¼šè·¨ä¾†æºé©—è­‰ç²å¾—æœ€é«˜æ¬Šé‡ï¼ˆ0.28ï¼‰

---

### âŒ No-Goï¼ˆä¸èƒ½é€² Phase 3ï¼‰

å¦‚æœå‡ºç¾ä»¥ä¸‹ä»»ä¸€æƒ…æ³ï¼ŒPhase 2 æœªå®Œæˆï¼š

- [ ] CCS èˆ‡ Entity Confidence æ··åœ¨ä¸€èµ·ï¼ˆæœƒè¢«æ¬Šå¨ç¶æ¶ï¼‰
- [ ] æ²’æœ‰ conflict handlingï¼ˆPhase 3 æœƒå…¨æ˜¯å‡è±¡ï¼‰
- [ ] Graph åªåšè¦–è¦ºåŒ–æ²’åšé¢¨éšªæŒ‡æ¨™ï¼ˆç„¡æ³•æª¢æ¸¬å­¤ç«‹ç­”æ¡ˆï¼‰
- [ ] Citation ç„¡æ³•ç¨ç«‹è©•åˆ†
- [ ] æ²’æœ‰æ˜ç¢ºçš„å¤±æ•ˆç‹€æ…‹å®šç¾©

---

## Phase 2 å®Œæˆç‹€æ…‹

### âœ… å·²å®Œæˆæ‰€æœ‰é—œéµçµ„ä»¶

**Phase 2-Aï¼šMachine-Readable Citations**
- Citation Object å®šç¾©èˆ‡å¼·åˆ¶çµæ§‹
- Evidence Type å®Œæ•´åˆ†é¡
- Verification Status ç‹€æ…‹æ©Ÿ
- Citation â†” AFB ç¶å®šè¦å‰‡
- Citation Failure States
- Citation ç”Ÿå‘½é€±æœŸç®¡ç†

**Phase 2-Bï¼šCitation Quality Evaluation**
- CCS 6 ç¶­è¨ˆç®—æ¨¡å‹
- ç¨ç«‹æ–¼ Entity Confidence
- Corroboration å„ªå…ˆè¨­è¨ˆ
- è¡çªè™•ç†ä¸‰æ®µå¼è¦å‰‡
- æ¬Šå¨ vs æ™‚æ•ˆæ€§æ¬Šè¡¡æ©Ÿåˆ¶

**Phase 2-Cï¼šEntity Graph.json**
- æœ€å° Graph Schemaï¼ˆ4 node + 3 edgeï¼‰
- Isolated Answer æª¢æ¸¬
- Single-Source Risk æª¢æ¸¬
- å®Œæ•´é¢¨éšªè©•ä¼°ç³»çµ±
- JSON åŒ¯å‡ºæ ¼å¼

---

## åŸ·è¡Œç‹€æ…‹

- âœ… Phase 0ï¼šEntity Optimization - å®Œæˆ
- âœ… Phase 1ï¼šAFB + E-E-A-T Signals - å®Œæˆ
- âœ… Phase 2-Aï¼šMachine-Readable Citations - å®Œæˆ
- âœ… Phase 2-Bï¼šCitation Quality Evaluation - å®Œæˆ
- âœ… Phase 2-Cï¼šEntity Graph.json - å®Œæˆ
- âœ… **Phase 2ï¼šå®Œæ•´æ”¶å°¾ï¼Œé€šé Go/No-Go é©—æ”¶**
- â³ Phase 3ï¼šReverse GEO Testing - æº–å‚™å•Ÿå‹•
- â³ Phase 4ï¼šSKILL.md + Scripts - å¾…å•Ÿå‹•

---

**Phase 2 æ­£å¼å®Œæˆã€‚ç³»çµ±å·²å…·å‚™ã€Œå¯è¨ˆç®—ã€å¯é©—è­‰ã€å¯æ‹’çµ•ã€çš„å®Œæ•´èƒ½åŠ›ã€‚**

---

**æ–‡æª”ç¶­è­·è€…**ï¼šAI Citation Engineering Team  
**æœ€å¾Œæ›´æ–°**ï¼š2026-02-06  
**Phase 2 ç‹€æ…‹**ï¼šâœ… å®Œæˆä¸¦é€šéé©—æ”¶
